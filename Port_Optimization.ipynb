{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7S5o621M9h8"
      },
      "source": [
        "**1. Imports & Configuration** This sets up our environment. We chose specific dates (2012-2020 for training) to ensure our model learns from different market regimes before being tested on the post-COVID era (2021-2024)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QPVt33HNKkK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import cvxpy as cp\n",
        "import joblib\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.covariance import LedoitWolf\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# We suppress specific LightGBM warnings to keep our backtest logs clean\n",
        "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "# We separate Training from Backtesting to prevent data leakage.\n",
        "PRICE_START    = \"2012-01-01\"\n",
        "TRAIN_END_DATE = \"2020-12-31\"   # ML Model sees data up to this point\n",
        "BACKTEST_START = \"2021-01-01\"   # Simulation runs on unseen data from here\n",
        "BACKTEST_END   = \"2024-12-31\"\n",
        "\n",
        "LOOKBACK_DAYS  = 252            # 1-Year rolling window for Covariance estimation\n",
        "REBALANCE_DAYS = 63             # Quarterly rebalancing to manage turnover costs\n",
        "MAX_WEIGHT     = 0.05           # 5% cap per asset to force diversification\n",
        "TOP_K          = 75             # Subset size for the optimizer to improve solver speed\n",
        "\n",
        "RANDOM_SEED    = 42\n",
        "SP500_HISTORY_CSV = \"resources/s&p500_history.csv\"   # PIT membership file\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "OUT_DATA_DIR     = \"resources/data\"\n",
        "OUT_MODELS_DIR   = \"resources/models\"\n",
        "OUT_RESULTS_DIR  = \"resources/results\"\n",
        "\n",
        "for d in [OUT_DATA_DIR, OUT_MODELS_DIR, OUT_RESULTS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "print(\"Environment configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGdDigbbNSxc"
      },
      "source": [
        "**2. ETF Universe Definition:** We augmented the S&P 500 universe with liquid ETFs. This was a strategic decision to ensure the optimizer always has \"safe harbor\" assets (like Bonds or Gold) available during high-volatility periods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWJRgQttLs6_"
      },
      "outputs": [],
      "source": [
        "# We include these liquid ETFs to ensure the optimizer has broad\n",
        "# asset classes available for hedging, not just single stocks.\n",
        "ETF_TICKERS = [\n",
        "    # Sector ETFs\n",
        "    'XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP',\n",
        "    'XLRE', 'XLU', 'XLY', 'XLC', 'XLV',\n",
        "    # Factor / Style ETFs\n",
        "    'SPLV', 'MTUM', 'QUAL', 'VLUE', 'IWM', 'VTV',\n",
        "    # Broad Market\n",
        "    'SPY', 'QQQ', 'DIA', 'VT', 'VTI',\n",
        "    # International / EM\n",
        "    'EFA', 'VWO', 'EWJ', 'EWU', 'EWC',\n",
        "    # Bonds (Critical for risk-off regimes)\n",
        "    'TLT', 'IEF', 'SHY', 'HYG', 'LQD',\n",
        "    # Commodities / Volatility\n",
        "    'GLD', 'SLV', 'DBC', 'USO', 'UUP', 'VIXY'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkB7mdmbNhpI"
      },
      "source": [
        "**3. Successor Mapping (Data Cleaning):** Data quality is paramount. We implemented this mapping to handle corporate actions (like mergers) that would otherwise look like \"delistings\" to the model. This prevents the backtester from selling a stock just because it changed its name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVfEsRUENlVy"
      },
      "outputs": [],
      "source": [
        "# We identified these specific corporate actions that broke our\n",
        "# data pipeline and mapped them to their modern tickers.\n",
        "SUCCESSOR_MAP = {\n",
        "    \"WFM\":  [\"AMZN\"],     \"TWC\":  [\"CMCSA\"],    \"COV\":  [\"MDT\"],\n",
        "    \"LVLT\": [\"LUMN\"],     \"CELG\": [\"BMY\"],      \"STJ\":  [\"ABT\"],\n",
        "    \"BXLT\": [\"TAK\"],      \"KRFT\": [\"KHC\"],      \"HNZ\":  [\"KHC\"],\n",
        "    \"PCP\":  [\"BRK-B\"],    \"DTV\":  [\"T\"],        \"SYMC\": [\"GEN\"],\n",
        "    \"RAI\":  [\"BTI\"],      \"BRCM\": [\"AVGO\"],     \"CVC\":  [\"CHTR\"],\n",
        "    \"GAS\":  [\"SO\"],       \"LLTC\": [\"ADI\"],      \"ARG\":  [\"APD\"],\n",
        "    \"SIAL\": [\"DHR\"],      \"NYX\":  [\"ICE\"],      \"FDO\":  [\"DLTR\"],\n",
        "    \"JOY\":  [\"CAT\"],      \"CVH\":  [\"AET\"],      \"PGN\":  [\"DUK\"],\n",
        "    \"APC\":  [\"OXY\"],      \"PXD\":  [\"XOM\"],      \"TWTR\": [\"X\"],\n",
        "    \"FB\":   [\"META\"],     \"MXIM\": [\"ADI\"],      \"ALTR\": [\"INTC\"],\n",
        "    \"BCR\":  [\"BDX\"],      \"CFN\":  [\"BDX\"],      \"GGP\":  [\"BAM\"],\n",
        "    \"WLTW\": [\"WTW\"],      \"HCBK\": [\"MTB\"],      \"PBCT\": [\"MTB\"],\n",
        "    \"WCG\":  [\"CNC\"],      \"MON\":  [\"BAYRY\"],    \"DISCK\":[\"WBD\"],\n",
        "    \"DISCA\":[\"WBD\"],      \"VIAB\": [\"PARA\"],     \"CBS\":  [\"PARA\"],\n",
        "    \"PARA\": [\"PARA\"],     \"HRS\":  [\"LHX\"],      \"RTN\":  [\"RTX\"],\n",
        "    \"UTX\":  [\"RTX\"],      \"ABMD\": [\"MDT\"],      \"ALXN\": [\"AZN\"],\n",
        "    \"DWDP\": [\"DOW\"],      \"XLNX\": [\"AMD\"],      \"AGN\":  [\"ABBV\"],\n",
        "    \"TIF\":  [\"MC.PA\"],    \"FRX\":  [\"AGN\"],      \"LO\":   [\"RAI\"],\n",
        "    \"GMCR\": [\"KDP\"],\n",
        "\n",
        "    # We explicitly drop these due to bankruptcy or privatization\n",
        "    \"APOL\": [], \"SPLS\": [], \"JCP\":  [], \"DF\":   [],\n",
        "    \"CHK\":  [], \"MNK\":  [], \"DNR\":  [],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xlju3I0NqDb"
      },
      "source": [
        "**4. Point-in-Time History Loading:** We load the historical S&P 500 composition. We normalize it using the map above to ensure that at any given date in the past, we are trading the correct universe of stocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XrVcp8ANqUF"
      },
      "outputs": [],
      "source": [
        "def load_sp500_history(path_csv):\n",
        "    \"\"\"Parses the raw CSV containing historical index membership.\"\"\"\n",
        "    if not os.path.exists(path_csv):\n",
        "        # We handle the missing file case for the demo\n",
        "        print(\"Warning: history file not found. Using dummy data for testing.\")\n",
        "        dates = pd.date_range(start=PRICE_START, end=BACKTEST_END, freq='M')\n",
        "        return pd.DataFrame({'date': dates, 'tickers': 'AAPL,MSFT,GOOG,AMZN'})\n",
        "\n",
        "    sp = pd.read_csv(path_csv)\n",
        "    sp[\"date\"] = pd.to_datetime(sp[\"date\"])\n",
        "    sp = sp.sort_values(\"date\").reset_index(drop=True)\n",
        "    return sp\n",
        "\n",
        "def normalize_sp500_with_successors(sp_df, mapping):\n",
        "    \"\"\"Applies our successor map to the raw history.\"\"\"\n",
        "    sp = sp_df.copy()\n",
        "\n",
        "    def map_row(ticker_str):\n",
        "        tickers = [t.strip() for t in ticker_str.split(\",\") if t.strip()]\n",
        "        mapped = []\n",
        "        for t in tickers:\n",
        "            if t in mapping and mapping[t]:\n",
        "                mapped.append(mapping[t][0]) # Use mapped successor\n",
        "            elif t in mapping and not mapping[t]:\n",
        "                continue # Skip delisted/bankrupt\n",
        "            else:\n",
        "                mapped.append(t)\n",
        "        mapped = sorted(set(mapped))\n",
        "        return \",\".join(mapped)\n",
        "\n",
        "    sp[\"tickers\"] = sp[\"tickers\"].apply(map_row)\n",
        "    return sp\n",
        "\n",
        "def get_sp500_members_on(date, sp_df):\n",
        "    \"\"\"Returns the valid investment universe for a specific date.\"\"\"\n",
        "    rows = sp_df[sp_df[\"date\"] <= date]\n",
        "    if rows.empty:\n",
        "        return []\n",
        "    last_row = rows.iloc[-1]\n",
        "    tickers = [t.strip() for t in last_row[\"tickers\"].split(\",\") if t.strip()]\n",
        "    return tickers\n",
        "\n",
        "# Load and Normalize\n",
        "sp500_df_raw = load_sp500_history(SP500_HISTORY_CSV)\n",
        "sp500_df = normalize_sp500_with_successors(sp500_df_raw, SUCCESSOR_MAP)\n",
        "sp500_df.to_csv(os.path.join(OUT_DATA_DIR, \"sp500_history_normalized.csv\"), index=False)\n",
        "print(\"S&P 500 History loaded and normalized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOIjS9h9N2be"
      },
      "source": [
        "**5. Price Downloads:** We download the price data for all assets. We included a specific fix for BRK.B because Yahoo Finance often rejects the dot notation, requiring a dash BRK-B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh6CjumAN4FS"
      },
      "outputs": [],
      "source": [
        "def build_universe_from_sp500(sp_df, start_date, end_date, extra_tickers=None):\n",
        "    start_date = pd.to_datetime(start_date)\n",
        "    end_date   = pd.to_datetime(end_date)\n",
        "    sub = sp_df[(sp_df[\"date\"] >= start_date) & (sp_df[\"date\"] <= end_date)]\n",
        "    all_tickers = set()\n",
        "    for row in sub[\"tickers\"]:\n",
        "        all_tickers |= set(t.strip() for t in row.split(\",\") if t.strip())\n",
        "    if extra_tickers:\n",
        "        all_tickers |= set(extra_tickers)\n",
        "    return sorted(all_tickers)\n",
        "\n",
        "def download_prices_yf(tickers, start_date, end_date):\n",
        "    print(f\"Downloading prices for {len(tickers)} tickers...\")\n",
        "    data = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
        "    if isinstance(data, pd.Series):\n",
        "        data = data.to_frame()\n",
        "    return data.sort_index()\n",
        "\n",
        "# Build Universe & Download\n",
        "universe_all = build_universe_from_sp500(sp500_df, PRICE_START, BACKTEST_END, ETF_TICKERS)\n",
        "prices_raw = download_prices_yf(universe_all, PRICE_START, BACKTEST_END)\n",
        "\n",
        "# Fix Ticker Formatting (e.g., BRK.B -> BRK-B)\n",
        "def fix_ticker_formatting(df):\n",
        "    fixed = {}\n",
        "    for t in df.columns:\n",
        "        if df[t].isna().all() and \".\" in t:\n",
        "            alt = t.replace(\".\", \"-\")\n",
        "            try:\n",
        "                # Quick probe to see if the dashed version exists\n",
        "                test = yf.download(alt, start=PRICE_START, end=BACKTEST_END, progress=False)[\"Close\"]\n",
        "                if not test.isna().all():\n",
        "                    fixed[t] = alt\n",
        "            except Exception: pass\n",
        "    return fixed\n",
        "\n",
        "fmt_map = fix_ticker_formatting(prices_raw)\n",
        "for old, new in fmt_map.items():\n",
        "    print(f\"[FORMAT FIX] {old} → {new}\")\n",
        "    prices_raw[new] = prices_raw[old]\n",
        "    del prices_raw[old]\n",
        "\n",
        "prices = prices_raw.dropna(axis=1, how=\"all\")\n",
        "prices.to_pickle(os.path.join(OUT_DATA_DIR, \"prices.pkl\"))\n",
        "print(\"Final Price Matrix Shape:\", prices.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsSl5F3EN9tC"
      },
      "source": [
        "**6. Feature Engineering:** Here we calculate our Alpha signals. We chose to target the Cross-Sectional Z-Score rather than raw returns. This neutralizes market beta—meaning our model learns to pick \"winners vs losers\" rather than just predicting \"everything goes up.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23m4_q_HN7aN"
      },
      "outputs": [],
      "source": [
        "def engineer_features(prices, horizon=21):\n",
        "    \"\"\"\n",
        "    We calculate technical factors including Momentum (20d), Volatility (10d),\n",
        "    and Mean Reversion (MA deviation).\n",
        "    Target: Cross-Sectional Z-Score of forward returns.\n",
        "    \"\"\"\n",
        "    prices_sorted = prices.copy().sort_index()\n",
        "    log_ret = np.log(prices_sorted / prices_sorted.shift(1))\n",
        "\n",
        "    records = []\n",
        "    for t in prices_sorted.columns:\n",
        "        s_price = prices_sorted[t]\n",
        "        s_log   = log_ret[t]\n",
        "\n",
        "        df_t = pd.DataFrame(index=prices_sorted.index)\n",
        "        df_t[\"ticker\"]       = t\n",
        "        df_t[\"log_ret_1d\"]   = s_log\n",
        "        df_t[\"log_ret_5d\"]   = s_log.rolling(5).sum()\n",
        "        df_t[\"ma_10d_dev\"]   = s_price.rolling(10).mean() / s_price - 1\n",
        "        df_t[\"vol_10d\"]      = s_log.rolling(10).std()\n",
        "        df_t[\"mom_20d\"]      = s_price.pct_change(20, fill_method=None)\n",
        "\n",
        "        # Forward target (next month's return)\n",
        "        df_t[\"target_raw\"] = s_log.shift(-horizon).rolling(horizon).sum()\n",
        "        records.append(df_t)\n",
        "\n",
        "    feats = pd.concat(records)\n",
        "    feats = feats.dropna()\n",
        "\n",
        "    # We use Z-scores to normalize across time\n",
        "    def cs_z(x):\n",
        "        return (x - x.mean()) / (x.std(ddof=0) + 1e-8)\n",
        "\n",
        "    feats[\"target\"] = feats.groupby(\"date\")[\"target_raw\"].transform(cs_z)\n",
        "    feats = feats.drop(columns=[\"target_raw\"])\n",
        "\n",
        "    return feats\n",
        "\n",
        "features_all = engineer_features(prices, horizon=21)\n",
        "features_all.to_pickle(os.path.join(OUT_DATA_DIR, \"features_all.pkl\"))\n",
        "\n",
        "# Chronological Split\n",
        "features_train = features_all[features_all.index <= TRAIN_END_DATE]\n",
        "features_test  = features_all[features_all.index > TRAIN_END_DATE]\n",
        "\n",
        "X_train = features_train.drop(columns=[\"target\"])\n",
        "y_train = features_train[\"target\"]\n",
        "print(f\"Features Generated. Training shape: {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bJIXMoEOFGW"
      },
      "source": [
        "**7. Model Training & Selection:** We define three candidate models (Ridge, XGBoost, LightGBM) and evaluate them using a chronological validation set. We automatically select the best model to use for the backtest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc1DoKf8OES2"
      },
      "outputs": [],
      "source": [
        "def build_ml_models():\n",
        "    \"\"\"Defines our candidate model architectures.\"\"\"\n",
        "    categorical = [\"ticker\"]\n",
        "    numeric = [\"log_ret_1d\", \"log_ret_5d\", \"ma_10d_dev\", \"vol_10d\", \"mom_20d\"]\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
        "            (\"num\", StandardScaler(), numeric),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    models = {\n",
        "        \"ridge\": Pipeline([(\"prep\", preprocessor), (\"model\", Ridge(alpha=1.0))]),\n",
        "        \"xgb\": Pipeline([(\"prep\", preprocessor), (\"model\", XGBRegressor(n_estimators=300, max_depth=6, n_jobs=-1, random_state=RANDOM_SEED))]),\n",
        "        \"lgbm\": Pipeline([(\"prep\", preprocessor), (\"model\", LGBMRegressor(n_estimators=300, num_leaves=31, random_state=RANDOM_SEED))])\n",
        "    }\n",
        "    return models\n",
        "\n",
        "def train_and_select_model(X_train, y_train, features_all, train_end_date):\n",
        "    # Validation Split (Last 20% of training data)\n",
        "    cutoff = int(len(X_train) * 0.8)\n",
        "    X_tr_sub, y_tr_sub = X_train.iloc[:cutoff], y_train.iloc[:cutoff]\n",
        "    X_val_sub, y_val_sub = X_train.iloc[cutoff:], y_train.iloc[cutoff:]\n",
        "\n",
        "    models = build_ml_models()\n",
        "    val_mse = {}\n",
        "\n",
        "    print(\"Evaluating models on validation set...\")\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_tr_sub.drop(columns=[\"target\"], errors='ignore'), y_tr_sub)\n",
        "        preds = model.predict(X_val_sub.drop(columns=[\"target\"], errors='ignore'))\n",
        "        mse = mean_squared_error(y_val_sub, preds)\n",
        "        val_mse[name] = mse\n",
        "        print(f\"Model {name} MSE: {mse:.6f}\")\n",
        "\n",
        "    best_name = min(val_mse, key=val_mse.get)\n",
        "    print(f\"Selected best model: {best_name}\")\n",
        "\n",
        "    # Retrain best model on full training data\n",
        "    best_model = models[best_name]\n",
        "    best_model.fit(X_train, y_train)\n",
        "\n",
        "    joblib.dump(best_model, os.path.join(OUT_MODELS_DIR, \"best_ml_model.joblib\"))\n",
        "    return best_model\n",
        "\n",
        "best_ml_model = train_and_select_model(X_train, y_train, features_all, TRAIN_END_DATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy-GMS2FOL2e"
      },
      "source": [
        "**8. Risk & Covariance Utilities:** To calculate the \"Mean-Variance\" optimization, we need a Covariance Matrix. We use Ledoit-Wolf shrinkage to make the matrix mathematically robust (Positive Semi-Definite) so our optimizer doesn't crash."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiLPYwpSOPji"
      },
      "outputs": [],
      "source": [
        "def make_psd(matrix, eps=1e-6):\n",
        "    \"\"\"\n",
        "    Forces covariance matrix to be Symmetric Positive Semi-Definite (PSD)\n",
        "    by clipping negative eigenvalues. Essential for CVXPY stability.\n",
        "    \"\"\"\n",
        "    sym = 0.5 * (matrix + matrix.T)\n",
        "    vals, vecs = np.linalg.eigh(sym)\n",
        "    vals_clipped = np.clip(vals, eps, None)\n",
        "    return vecs @ np.diag(vals_clipped) @ vecs.T\n",
        "\n",
        "def compute_mu_cov_for_universe(prices, active_tickers, reb_date, lookback_days=252):\n",
        "    \"\"\"Computes robust Mu (Expected Return) and Sigma (Covariance) using Ledoit-Wolf.\"\"\"\n",
        "    px = prices[active_tickers].loc[:reb_date].tail(lookback_days)\n",
        "    log_ret = np.log(px / px.shift(1)).dropna(how=\"any\")\n",
        "\n",
        "    if log_ret.shape[0] < 10 or log_ret.shape[1] < 5:\n",
        "        return None, None, []\n",
        "\n",
        "    mu = log_ret.mean().values\n",
        "    lw = LedoitWolf().fit(log_ret.values) # Shrinkage estimation\n",
        "    cov_psd = make_psd(lw.covariance_)\n",
        "\n",
        "    return mu, cov_psd, log_ret.columns.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM6wzxH9OSm0"
      },
      "source": [
        "**9. Optimization Engine:** This is the core mathematical engine. We use cvxpy to solve for the weights that maximize return minus risk, subject to constraints (no shorting, max 5% per stock)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPbvotncOUkp"
      },
      "outputs": [],
      "source": [
        "def mean_variance_opt_full(mu, cov, max_weight=0.05, risk_aversion=1.0):\n",
        "    \"\"\"\n",
        "    Solves: maximize (mu.T @ w) - gamma * (w.T @ cov @ w)\n",
        "    Subject to: sum(w)=1, 0 <= w <= max_weight\n",
        "    \"\"\"\n",
        "    n = len(mu)\n",
        "    w = cp.Variable(n)\n",
        "\n",
        "    # We wrap in psd_wrap to assure the solver the matrix is safe\n",
        "    cov_psd = cp.psd_wrap(cov)\n",
        "\n",
        "    objective = cp.Maximize(mu @ w - risk_aversion * cp.quad_form(w, cov_psd))\n",
        "    constraints = [cp.sum(w) == 1, w >= 0, w <= max_weight]\n",
        "\n",
        "    prob = cp.Problem(objective, constraints)\n",
        "    try:\n",
        "        prob.solve(solver=cp.SCS, verbose=False)\n",
        "    except:\n",
        "        return np.ones(n) / n # Fallback to Equal Weight on failure\n",
        "\n",
        "    if w.value is None: return np.ones(n) / n\n",
        "\n",
        "    # Clean up small numerical noise\n",
        "    w_val = np.clip(np.array(w.value).ravel(), 0, None)\n",
        "    return w_val / w_val.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z6aX0BqOYkh"
      },
      "source": [
        "**10. Monte Carlo Simulation:** For every single rebalance, we run a simulation to see \"what could happen\" over the next quarter. This gives us our risk metrics (VaR/CVaR)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j41YHPnsOWwO"
      },
      "outputs": [],
      "source": [
        "def monte_carlo_sim(mu, cov, weights, horizon=21, n_sims=10000):\n",
        "    \"\"\"\n",
        "    Simulates thousands of potential future price paths to estimate risk.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate random market scenarios\n",
        "        sims = np.random.multivariate_normal(mu, cov, size=(n_sims, horizon))\n",
        "    except np.linalg.LinAlgError:\n",
        "        # Fallback for numerical instability\n",
        "        sims = np.random.multivariate_normal(mu, np.diag(np.diag(cov)), size=(n_sims, horizon))\n",
        "\n",
        "    port_daily = sims @ weights\n",
        "    # Calculate cumulative return for every path\n",
        "    final_returns = np.exp(np.cumsum(port_daily, axis=1))[:, -1] - 1\n",
        "    return final_returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRcfXl9IOceT"
      },
      "source": [
        "**11. Visualization Utilities:** We separated the plotting logic to keep the backtest loops clean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cm8Lh2eOfaS"
      },
      "outputs": [],
      "source": [
        "def plot_mc_sample(mc_returns, out_dir, tag):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(mc_returns, bins=100, density=True, alpha=0.7)\n",
        "    plt.title(f\"Monte Carlo Risk Distribution ({tag})\")\n",
        "    plt.xlabel(\"Simulated Return\")\n",
        "    plt.savefig(os.path.join(out_dir, f\"mc_dist_{tag}.pdf\"))\n",
        "    plt.show()\n",
        "\n",
        "def plot_final_weights(weights_hist, name, out_dir):\n",
        "    if not weights_hist: return\n",
        "    # Extract last rebalance weights\n",
        "    last_w = weights_hist[-1][1].sort_values(ascending=False).head(20)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    last_w.plot(kind=\"bar\")\n",
        "    plt.title(f\"{name}: Top 20 Holdings (Final Rebalance)\")\n",
        "    plt.savefig(os.path.join(out_dir, f\"{name}_weights.pdf\"))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgkRuLQlOh6m"
      },
      "source": [
        "**12. Backtesting Functions:** Here we define the logic for our three strategies: Equal Weight, MVO-Historical, and MVO-ML. The ML strategy blends the model's alpha score with the historical mean to avoid making extreme bets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvPP5IF6Ohrt"
      },
      "outputs": [],
      "source": [
        "def run_backtest(strategy_type, prices, sp500_df, start_date, end_date, ml_model=None, features=None):\n",
        "    \"\"\"Unified backtesting engine for all 3 strategies.\"\"\"\n",
        "    prices_bt = prices.loc[start_date:end_date]\n",
        "    dates = prices_bt.index\n",
        "    capital = 1.0\n",
        "    vals, weights_hist = [], []\n",
        "\n",
        "    i = 0\n",
        "    print(f\"--- Running Backtest: {strategy_type} ---\")\n",
        "\n",
        "    while i < len(dates):\n",
        "        reb_date = dates[i]\n",
        "\n",
        "        # 1. Define Universe (PIT)\n",
        "        sp_members = get_sp500_members_on(reb_date, sp500_df)\n",
        "        universe = [t for t in sp_members if t in prices_bt.columns and prices_bt[t].notna().sum() > LOOKBACK_DAYS]\n",
        "\n",
        "        # 2. Estimate Risk Model\n",
        "        mu_hist, cov, tickers = compute_mu_cov_for_universe(prices_bt, universe, reb_date)\n",
        "\n",
        "        if cov is None:\n",
        "            i += REBALANCE_DAYS\n",
        "            continue\n",
        "\n",
        "        # 3. Optimize Weights based on Strategy\n",
        "        if strategy_type == \"Equal\":\n",
        "            w = np.ones(len(tickers)) / len(tickers)\n",
        "\n",
        "        elif strategy_type == \"MV-Hist\":\n",
        "            w = mean_variance_opt_full(mu_hist, cov)\n",
        "\n",
        "        elif strategy_type == \"MV-ML\":\n",
        "            # Filter features for current date\n",
        "            current_feats = features.loc[features.index <= reb_date].groupby('ticker').last()\n",
        "            valid_tickers = [t for t in tickers if t in current_feats.index]\n",
        "\n",
        "            # Predict Alpha\n",
        "            preds = ml_model.predict(current_feats.loc[valid_tickers].drop(columns=['target'], errors='ignore'))\n",
        "\n",
        "            # Blend ML Alpha with Historical Mu (Bayesian-style shrinkage)\n",
        "            # This prevents the model from making wild bets on low-confidence predictions\n",
        "            alpha_vec = pd.Series(preds, index=valid_tickers).reindex(tickers).fillna(0).values\n",
        "            mu_blended = 0.4 * alpha_vec + 0.6 * mu_hist\n",
        "\n",
        "            w = mean_variance_opt_full(mu_blended, cov)\n",
        "\n",
        "        # 4. Simulate Forward Performance\n",
        "        hold_days = min(REBALANCE_DAYS, len(dates) - i)\n",
        "        px_slice = prices_bt.iloc[i : i + hold_days][tickers]\n",
        "        ret_slice = np.log(px_slice / px_slice.shift(1)).dropna()\n",
        "\n",
        "        port_ret = ret_slice @ w\n",
        "        for r in port_ret:\n",
        "            capital *= np.exp(r)\n",
        "            vals.append(capital)\n",
        "\n",
        "        weights_hist.append((reb_date, pd.Series(w, index=tickers)))\n",
        "\n",
        "        # Plot MC Risk for the first rebalance only\n",
        "        if i == 0:\n",
        "            mc_ret = monte_carlo_sim(mu_hist, cov, w)\n",
        "            plot_mc_sample(mc_ret, OUT_RESULTS_DIR, strategy_type)\n",
        "\n",
        "        i += REBALANCE_DAYS\n",
        "\n",
        "    return pd.DataFrame({'portfolio': vals}, index=dates[:len(vals)]), weights_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4KYNYKQOoHf"
      },
      "source": [
        "**13. Execution:** We run all three backtests sequentially. This takes the heavy lifting defined above and actually executes it on the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQ9zMOIsOrUo"
      },
      "outputs": [],
      "source": [
        "# Run the Backtests\n",
        "port_eq, w_eq = run_backtest(\"Equal\", prices, sp500_df, BACKTEST_START, BACKTEST_END)\n",
        "port_hist, w_hist = run_backtest(\"MV-Hist\", prices, sp500_df, BACKTEST_START, BACKTEST_END)\n",
        "port_ml, w_ml = run_backtest(\"MV-ML\", prices, sp500_df, BACKTEST_START, BACKTEST_END,\n",
        "                             ml_model=best_ml_model, features=features_all)\n",
        "\n",
        "# Save Results\n",
        "port_eq.to_csv(os.path.join(OUT_RESULTS_DIR, \"port_eq.csv\"))\n",
        "port_hist.to_csv(os.path.join(OUT_RESULTS_DIR, \"port_hist.csv\"))\n",
        "port_ml.to_csv(os.path.join(OUT_RESULTS_DIR, \"port_ml.csv\"))\n",
        "print(\"Backtests Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqDUAs0lOtiI"
      },
      "source": [
        "**14. Final Comparison & Stats:** We compile the equity curves into one chart and calculate the final Sharpe Ratios to see which strategy won."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6QPZVP9OtN7"
      },
      "outputs": [],
      "source": [
        "def compute_stats(df, name):\n",
        "    ret = df['portfolio'].pct_change().dropna()\n",
        "    ann_ret = ret.mean() * 252\n",
        "    ann_vol = ret.std() * np.sqrt(252)\n",
        "    sharpe = ann_ret / ann_vol\n",
        "    return {\"Strategy\": name, \"Sharpe\": sharpe, \"Vol\": ann_vol, \"Return\": ann_ret}\n",
        "\n",
        "stats = pd.DataFrame([\n",
        "    compute_stats(port_eq, \"Equal Weight\"),\n",
        "    compute_stats(port_hist, \"MV-Hist\"),\n",
        "    compute_stats(port_ml, \"MV-ML\")\n",
        "]).set_index(\"Strategy\")\n",
        "\n",
        "print(\"--- Final Performance Stats ---\")\n",
        "display(stats)\n",
        "\n",
        "# Plot Equity Curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(port_eq, label=\"Equal Weight\")\n",
        "plt.plot(port_hist, label=\"MV-Hist\")\n",
        "plt.plot(port_ml, label=\"MV-ML\")\n",
        "plt.title(\"Cumulative Portfolio Performance (2021-2024)\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(OUT_RESULTS_DIR, \"final_comparison.pdf\"))\n",
        "plt.show()\n",
        "\n",
        "# Show Top Holdings\n",
        "plot_final_weights(w_ml, \"MV-ML\", OUT_RESULTS_DIR)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
