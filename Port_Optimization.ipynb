{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyqZyhhrxWX_"
      },
      "source": [
        "**Setup:**\n",
        "Import all libraries, set basic config (dates, paths, random seed), and define the ETF universe and successor map for old tickers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiDiIIxMxWAt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import cvxpy as cp\n",
        "import joblib\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.covariance import LedoitWolf\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Quiet down some LightGBM feature-name warnings that clutter the log\n",
        "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
        "\n",
        "\n",
        "\n",
        "# Date ranges for training vs backtest\n",
        "PRICE_START    = \"2012-01-01\"\n",
        "TRAIN_END_DATE = \"2020-12-31\"   # ML only sees data up to this date\n",
        "BACKTEST_START = \"2021-01-01\"\n",
        "BACKTEST_END   = \"2024-12-31\"\n",
        "\n",
        "# General backtest / risk model settings\n",
        "LOOKBACK_DAYS  = 252            # how many days of history to estimate μ, Σ\n",
        "REBALANCE_DAYS = 63             # roughly quarterly rebalancing (in trading days)\n",
        "MAX_WEIGHT     = 0.05           # hard cap per asset\n",
        "TOP_K          = 75             # max number of names ML is allowed to pick\n",
        "\n",
        "RANDOM_SEED    = 42\n",
        "\n",
        "# Input / output paths (expect these folders to exist or be created)\n",
        "SP500_HISTORY_CSV = \"resources/s&p500_history.csv\"   # point-in-time membership\n",
        "\n",
        "OUT_DATA_DIR     = \"resources/data\"\n",
        "OUT_MODELS_DIR   = \"resources/models\"\n",
        "OUT_RESULTS_DIR  = \"resources/results\"\n",
        "\n",
        "os.makedirs(OUT_DATA_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Extra ETFs added on top of S&P names to give the optimizer more tools\n",
        "ETF_TICKERS = [\n",
        "    # Sector ETFs\n",
        "    'XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP',\n",
        "    'XLRE', 'XLU', 'XLY', 'XLC', 'XLV',\n",
        "\n",
        "    # Factor / Style ETFs\n",
        "    'SPLV', 'MTUM', 'QUAL', 'VLUE', 'IWM', 'VTV',\n",
        "\n",
        "    # Broad Market ETFs\n",
        "    'SPY', 'QQQ', 'DIA', 'VT', 'VTI',\n",
        "\n",
        "    # International / EM\n",
        "    'EFA', 'VWO', 'EWJ', 'EWU', 'EWC',\n",
        "\n",
        "    # Bonds\n",
        "    'TLT', 'IEF', 'SHY', 'HYG', 'LQD',\n",
        "\n",
        "    # Commodities / Alternatives\n",
        "    'GLD', 'SLV', 'DBC', 'USO', 'UUP',\n",
        "\n",
        "    # Volatility\n",
        "    'VIXY'\n",
        "]\n",
        "\n",
        "\n",
        "# Map delisted tickers to their modern replacements when possible.\n",
        "# If the list is empty, we basically drop that old name.\n",
        "SUCCESSOR_MAP = {\n",
        "    # Well-known acquisitions / mergers\n",
        "    \"WFM\":  [\"AMZN\"],\n",
        "    \"TWC\":  [\"CMCSA\"],\n",
        "    \"COV\":  [\"MDT\"],\n",
        "    \"LVLT\": [\"LUMN\"],\n",
        "    \"CELG\": [\"BMY\"],\n",
        "    \"STJ\":  [\"ABT\"],\n",
        "    \"BXLT\": [\"TAK\"],\n",
        "    \"KRFT\": [\"KHC\"],\n",
        "    \"HNZ\":  [\"KHC\"],\n",
        "    \"PCP\":  [\"BRK-B\"],\n",
        "    \"DTV\":  [\"T\"],\n",
        "    \"SYMC\": [\"GEN\"],\n",
        "    \"RAI\":  [\"BTI\"],\n",
        "    \"BRCM\": [\"AVGO\"],\n",
        "    \"CVC\":  [\"CHTR\"],\n",
        "    \"GAS\":  [\"SO\"],\n",
        "    \"LLTC\": [\"ADI\"],\n",
        "    \"ARG\":  [\"APD\"],\n",
        "    \"SIAL\": [\"DHR\"],\n",
        "    \"NYX\":  [\"ICE\"],\n",
        "    \"FDO\":  [\"DLTR\"],\n",
        "    \"JOY\":  [\"CAT\"],\n",
        "    \"CVH\":  [\"AET\"],\n",
        "    \"PGN\":  [\"DUK\"],\n",
        "    \"APC\":  [\"OXY\"],\n",
        "    \"PXD\":  [\"XOM\"],\n",
        "    \"TWTR\": [\"X\"],\n",
        "    \"FB\":   [\"META\"],\n",
        "    \"MXIM\": [\"ADI\"],\n",
        "    \"ALTR\": [\"INTC\"],\n",
        "    \"BCR\":  [\"BDX\"],\n",
        "    \"CFN\":  [\"BDX\"],\n",
        "    \"GGP\":  [\"BAM\"],\n",
        "    \"WLTW\": [\"WTW\"],\n",
        "    \"HCBK\": [\"MTB\"],\n",
        "    \"PBCT\": [\"MTB\"],\n",
        "    \"WCG\":  [\"CNC\"],\n",
        "    \"MON\":  [\"BAYRY\"],\n",
        "    \"DISCK\":[\"WBD\"],\n",
        "    \"DISCA\":[\"WBD\"],\n",
        "    \"VIAB\": [\"PARA\"],\n",
        "    \"CBS\":  [\"PARA\"],\n",
        "    \"PARA\": [\"PARA\"],\n",
        "    \"HRS\":  [\"LHX\"],\n",
        "    \"RTN\":  [\"RTX\"],\n",
        "    \"UTX\":  [\"RTX\"],\n",
        "    \"ABMD\": [\"MDT\"],\n",
        "    \"ALXN\": [\"AZN\"],\n",
        "    \"DWDP\": [\"DOW\"],\n",
        "    \"XLNX\": [\"AMD\"],\n",
        "    \"AGN\":  [\"ABBV\"],\n",
        "    \"TIF\":  [\"MC.PA\"],\n",
        "    \"FRX\":  [\"AGN\"],\n",
        "    \"LO\":   [\"RAI\"],\n",
        "    \"GMCR\": [\"KDP\"],\n",
        "\n",
        "    # Explicit drops (old / OTC / bankrupt)\n",
        "    \"APOL\": [],\n",
        "    \"SPLS\": [],\n",
        "    \"JCP\":  [],\n",
        "    \"DF\":   [],\n",
        "    \"CHK\":  [],\n",
        "    \"MNK\":  [],\n",
        "    \"DNR\":  [],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImUWp6xLxiw7"
      },
      "source": [
        "**S&P 500 membership helpers:**\n",
        "Load the point-in-time S&P 500 membership, apply the successor mapping, and build a universe between two dates (plus the ETF tickers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8bxfjlaxogH"
      },
      "outputs": [],
      "source": [
        "def load_sp500_history(path_csv):\n",
        "    \"\"\"\n",
        "    Load the SP500 history CSV.\n",
        "    The file should have: ['date', 'tickers'] where 'tickers' is a comma-separated list.\n",
        "    \"\"\"\n",
        "    sp = pd.read_csv(path_csv)\n",
        "    sp[\"date\"] = pd.to_datetime(sp[\"date\"])\n",
        "    sp = sp.sort_values(\"date\").reset_index(drop=True)\n",
        "    return sp\n",
        "\n",
        "def normalize_sp500_with_successors(sp_df, mapping):\n",
        "    \"\"\"\n",
        "    Apply the successor map:\n",
        "    - replace old tickers with new ones where possible\n",
        "    - drop tickers explicitly mapped to []\n",
        "    - remove duplicates on each date\n",
        "    \"\"\"\n",
        "    sp = sp_df.copy()\n",
        "\n",
        "    def map_row(ticker_str):\n",
        "        tickers = [t.strip() for t in ticker_str.split(\",\") if t.strip()]\n",
        "        mapped = []\n",
        "        for t in tickers:\n",
        "            if t in mapping and mapping[t]:\n",
        "                mapped.append(mapping[t][0])  # use the first mapped symbol\n",
        "            elif t in mapping and not mapping[t]:\n",
        "                # this one is meant to be dropped completely\n",
        "                continue\n",
        "            else:\n",
        "                mapped.append(t)\n",
        "        mapped = sorted(set(mapped))\n",
        "        return \",\".join(mapped)\n",
        "\n",
        "    sp[\"tickers\"] = sp[\"tickers\"].apply(map_row)\n",
        "    return sp\n",
        "\n",
        "def get_sp500_members_on(date, sp_df):\n",
        "    \"\"\"\n",
        "    Get the S&P 500 members as of a given date (point-in-time).\n",
        "    \"\"\"\n",
        "    rows = sp_df[sp_df[\"date\"] <= date]\n",
        "    if rows.empty:\n",
        "        return []\n",
        "    last_row = rows.iloc[-1]\n",
        "    tickers = [t.strip() for t in last_row[\"tickers\"].split(\",\") if t.strip()]\n",
        "    return tickers\n",
        "\n",
        "# Load and normalize the S&P history\n",
        "sp500_df_raw = load_sp500_history(SP500_HISTORY_CSV)\n",
        "sp500_df = normalize_sp500_with_successors(sp500_df_raw, SUCCESSOR_MAP)\n",
        "\n",
        "# Save normalized PIT file so we can re-use it later if needed\n",
        "sp500_df.to_csv(os.path.join(OUT_DATA_DIR, \"sp500_history_normalized.csv\"), index=False)\n",
        "\n",
        "def build_universe_from_sp500(sp_df, start_date, end_date, extra_tickers=None):\n",
        "    \"\"\"\n",
        "    Build the full trading universe across a date range,\n",
        "    optionally adding extra tickers (like ETFs).\n",
        "    \"\"\"\n",
        "    start_date = pd.to_datetime(start_date)\n",
        "    end_date   = pd.to_datetime(end_date)\n",
        "\n",
        "    sub = sp_df[(sp_df[\"date\"] >= start_date) & (sp_df[\"date\"] <= end_date)]\n",
        "    all_tickers = set()\n",
        "    for row in sub[\"tickers\"]:\n",
        "        all_tickers |= set(t.strip() for t in row.split(\",\") if t.strip())\n",
        "\n",
        "    if extra_tickers:\n",
        "        all_tickers |= set(extra_tickers)\n",
        "\n",
        "    return sorted(all_tickers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYfCB2GXxueq"
      },
      "source": [
        "**Download prices from Yahoo Finance:**\n",
        "Build the full universe, pull daily close prices from yfinance, fix odd tickers like BRK.B → BRK-B, and store the final price matrix to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkPco3mvxwOn"
      },
      "outputs": [],
      "source": [
        "def download_prices_yf(tickers, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Download adjusted close prices from Yahoo Finance for all tickers.\n",
        "    \"\"\"\n",
        "    print(f\"Downloading prices for {len(tickers)} tickers...\")\n",
        "    data = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
        "    if isinstance(data, pd.Series):  # handle single-ticker edge case\n",
        "        data = data.to_frame()\n",
        "    data = data.sort_index()\n",
        "    print(\"Raw price shape:\", data.shape)\n",
        "    return data\n",
        "\n",
        "# Build the combined universe (S&P + ETFs)\n",
        "universe_all = build_universe_from_sp500(\n",
        "    sp500_df,\n",
        "    PRICE_START,\n",
        "    BACKTEST_END,\n",
        "    extra_tickers=ETF_TICKERS\n",
        ")\n",
        "\n",
        "prices_raw = download_prices_yf(universe_all, PRICE_START, BACKTEST_END)\n",
        "\n",
        "# Small helper: try switching BRK.B-style tickers to BRK-B, etc. if the dot version is all NaN\n",
        "def fix_ticker_formatting(df):\n",
        "    fixed = {}\n",
        "    for t in df.columns:\n",
        "        if df[t].isna().all() and \".\" in t:\n",
        "            alt = t.replace(\".\", \"-\")\n",
        "            try:\n",
        "                test = yf.download(alt, start=PRICE_START, end=BACKTEST_END)[\"Close\"]\n",
        "                if not test.isna().all():\n",
        "                    fixed[t] = alt\n",
        "            except Exception:\n",
        "                pass\n",
        "    return fixed\n",
        "\n",
        "fmt_map = fix_ticker_formatting(prices_raw)\n",
        "for old, new in fmt_map.items():\n",
        "    print(f\"[FORMAT FIX] {old} → {new}\")\n",
        "    prices_raw[new] = prices_raw[old]\n",
        "    del prices_raw[old]\n",
        "\n",
        "# Drop columns that never had any prices\n",
        "prices = prices_raw.dropna(axis=1, how=\"all\")\n",
        "print(\"Final price DataFrame shape:\", prices.shape)\n",
        "\n",
        "# Save cleaned prices for reuse\n",
        "prices.to_pickle(os.path.join(OUT_DATA_DIR, \"prices.pkl\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi_hB3QGx13i"
      },
      "source": [
        "**Build ML features:**\n",
        "Turn prices into per-ticker features (returns, volatility, momentum) and a cross-sectional z-scored target, then split into train vs test by date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xXWT5cCx4cc"
      },
      "outputs": [],
      "source": [
        "def engineer_features(prices, horizon=21):\n",
        "    \"\"\"\n",
        "    Create basic technical features per ticker and a cross-sectional target.\n",
        "    - Features: short/medium returns, 10d moving-average deviation, 10d vol, 20d momentum.\n",
        "    - Target: sum of log returns over the next `horizon` days, z-scored cross-sectionally by date.\n",
        "    \"\"\"\n",
        "    prices_sorted = prices.copy().sort_index()\n",
        "    log_ret = np.log(prices_sorted / prices_sorted.shift(1))\n",
        "\n",
        "    records = []\n",
        "\n",
        "    for t in prices_sorted.columns:\n",
        "        s_price = prices_sorted[t]\n",
        "        s_log   = log_ret[t]\n",
        "\n",
        "        df_t = pd.DataFrame(index=prices_sorted.index)\n",
        "        df_t[\"ticker\"]       = t\n",
        "        df_t[\"log_ret_1d\"]   = s_log\n",
        "        df_t[\"log_ret_5d\"]   = s_log.rolling(5).sum()\n",
        "        df_t[\"ma_10d_dev\"]   = s_price.rolling(10).mean() / s_price - 1\n",
        "        df_t[\"vol_10d\"]      = s_log.rolling(10).std()\n",
        "        df_t[\"mom_20d\"]      = s_price.pct_change(20, fill_method=None)\n",
        "\n",
        "        # Future `horizon`-day log return\n",
        "        df_t[\"target_raw\"] = s_log.shift(-horizon).rolling(horizon).sum()\n",
        "\n",
        "        records.append(df_t)\n",
        "\n",
        "    feats = pd.concat(records)\n",
        "    feats.index.name = \"date\"\n",
        "    feats = feats.dropna(subset=[\"target_raw\",\n",
        "                                 \"log_ret_1d\",\n",
        "                                 \"log_ret_5d\",\n",
        "                                 \"ma_10d_dev\",\n",
        "                                 \"vol_10d\",\n",
        "                                 \"mom_20d\"])\n",
        "\n",
        "    # Cross-sectional z-score per date\n",
        "    def cs_z(x):\n",
        "        mu = x.mean()\n",
        "        sd = x.std(ddof=0)\n",
        "        return (x - mu) / (sd + 1e-8)\n",
        "\n",
        "    feats[\"target\"] = feats.groupby(\"date\")[\"target_raw\"].transform(cs_z)\n",
        "    feats = feats.drop(columns=[\"target_raw\"])\n",
        "    feats = feats.dropna(subset=[\"target\"])\n",
        "\n",
        "    print(\"Feature matrix shape:\", feats.shape)\n",
        "    return feats\n",
        "\n",
        "features_all = engineer_features(prices, horizon=21)\n",
        "features_all.to_pickle(os.path.join(OUT_DATA_DIR, \"features_all.pkl\"))\n",
        "\n",
        "# Train / backtest split by date (so we don't leak post-2020 info)\n",
        "features_train = features_all[features_all.index <= TRAIN_END_DATE]\n",
        "features_test  = features_all[features_all.index > TRAIN_END_DATE]\n",
        "\n",
        "X_train = features_train.drop(columns=[\"target\"])\n",
        "y_train = features_train[\"target\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgTx8y9yx7bU"
      },
      "source": [
        "**Define ML models and pick the best one:**\n",
        "Set up Ridge, XGBoost, and LightGBM pipelines with shared preprocessing, do a time-based validation split, pick the lowest-MSE model, and save it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEz4vVHfx_3T"
      },
      "outputs": [],
      "source": [
        "def build_ml_models():\n",
        "    \"\"\"\n",
        "    Build three ML models (Ridge, XGBoost, LightGBM) that all share the same preprocessing:\n",
        "    - one-hot ticker\n",
        "    - standardize numeric features\n",
        "    \"\"\"\n",
        "    categorical = [\"ticker\"]\n",
        "    numeric = [\"log_ret_1d\", \"log_ret_5d\", \"ma_10d_dev\", \"vol_10d\", \"mom_20d\"]\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
        "            (\"num\", StandardScaler(), numeric),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # 1) Ridge baseline\n",
        "    ridge = Pipeline([\n",
        "        (\"prep\", preprocessor),\n",
        "        (\"model\", Ridge(alpha=1.0))\n",
        "    ])\n",
        "\n",
        "    # 2) XGBoost\n",
        "    xgb = Pipeline([\n",
        "        (\"prep\", preprocessor),\n",
        "        (\"model\", XGBRegressor(\n",
        "            n_estimators=300,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=6,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            objective=\"reg:squarederror\",\n",
        "            n_jobs=-1,\n",
        "            reg_lambda=1.0,\n",
        "            random_state=RANDOM_SEED\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # 3) LightGBM\n",
        "    lgbm = Pipeline([\n",
        "        (\"prep\", preprocessor),\n",
        "        (\"model\", LGBMRegressor(\n",
        "            n_estimators=300,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=-1,\n",
        "            num_leaves=31,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            objective=\"regression\",\n",
        "            random_state=RANDOM_SEED\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    models = {\n",
        "        \"ridge\": ridge,\n",
        "        \"xgb\": xgb,\n",
        "        \"lgbm\": lgbm\n",
        "    }\n",
        "\n",
        "    return models\n",
        "\n",
        "def train_and_select_model(X_train, y_train, features_all, train_end_date, val_frac=0.2):\n",
        "    \"\"\"\n",
        "    Train all models and choose the best one by validation MSE.\n",
        "    Validation split is done chronologically within the training window.\n",
        "    \"\"\"\n",
        "    feats_train = features_all[features_all.index <= train_end_date]\n",
        "    n = len(feats_train)\n",
        "    cutoff = int((1 - val_frac) * n)\n",
        "    feats_train_sub = feats_train.iloc[:cutoff]\n",
        "    feats_val_sub   = feats_train.iloc[cutoff:]\n",
        "\n",
        "    X_train_sub = feats_train_sub.drop(columns=[\"target\"])\n",
        "    y_train_sub = feats_train_sub[\"target\"]\n",
        "    X_val_sub   = feats_val_sub.drop(columns=[\"target\"])\n",
        "    y_val_sub   = feats_val_sub[\"target\"]\n",
        "\n",
        "    print(f\"Train subset: {len(X_train_sub):,} rows, Val subset: {len(X_val_sub):,} rows\")\n",
        "\n",
        "    models = build_ml_models()\n",
        "    val_mse = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nFitting {name} model...\")\n",
        "        model.fit(X_train_sub, y_train_sub)\n",
        "        preds = model.predict(X_val_sub)\n",
        "        mse = mean_squared_error(y_val_sub, preds)\n",
        "        val_mse[name] = mse\n",
        "        print(f\"{name} validation MSE: {mse:.6f}\")\n",
        "\n",
        "    best_name = min(val_mse, key=val_mse.get)\n",
        "    best_model = models[best_name]\n",
        "    print(\"\\nSelected best model:\", best_name, \"with MSE =\", val_mse[best_name])\n",
        "\n",
        "    print(f\"Refitting best model '{best_name}' on full training data...\")\n",
        "    best_model.fit(X_train, y_train)\n",
        "\n",
        "    # Save chosen model, plus all candidates and the validation scores\n",
        "    joblib.dump(best_model, os.path.join(OUT_MODELS_DIR, \"best_ml_model.joblib\"))\n",
        "    joblib.dump(models,      os.path.join(OUT_MODELS_DIR, \"all_ml_models.joblib\"))\n",
        "    pd.Series(val_mse).to_csv(os.path.join(OUT_RESULTS_DIR, \"ml_model_validation_mse.csv\"))\n",
        "\n",
        "    return best_model, val_mse\n",
        "\n",
        "print(\"Fitting ML models and selecting best...\")\n",
        "best_ml_model, val_mse = train_and_select_model(\n",
        "    X_train, y_train, features_all, TRAIN_END_DATE\n",
        ")\n",
        "print(\"Done. Best model saved.\")\n",
        "\n",
        "# Reload from disk once just to prove the save/load works\n",
        "best_ml_model = joblib.load(os.path.join(OUT_MODELS_DIR, \"best_ml_model.joblib\"))\n",
        "print(\"Loaded best_ml_model from disk.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH8bih9gyEhA"
      },
      "source": [
        "**Risk model & Monte Carlo helpers:**\n",
        "Build covariance matrices (with shrinkage + PSD fix), solve mean-variance optimization, and define Monte Carlo simulation + some plotting helpers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr2FPRHnyHmJ"
      },
      "outputs": [],
      "source": [
        "def make_psd(matrix, eps=1e-6):\n",
        "    \"\"\"\n",
        "    Make a covariance matrix numerically safe (symmetric PSD) by clipping\n",
        "    small/negative eigenvalues up to `eps`.\n",
        "    \"\"\"\n",
        "    sym = 0.5 * (matrix + matrix.T)\n",
        "    vals, vecs = np.linalg.eigh(sym)\n",
        "    vals_clipped = np.clip(vals, eps, None)\n",
        "    psd = vecs @ np.diag(vals_clipped) @ vecs.T\n",
        "    return psd\n",
        "\n",
        "def compute_mu_cov_for_universe(prices, active_tickers, reb_date, lookback_days=252):\n",
        "    \"\"\"\n",
        "    Estimate μ and Σ for a given universe using the last `lookback_days`.\n",
        "    Uses Ledoit-Wolf shrinkage, then fixes the covariance to be PSD.\n",
        "    \"\"\"\n",
        "    px = prices[active_tickers].loc[:reb_date]\n",
        "    px_window = px.tail(lookback_days)\n",
        "\n",
        "    log_ret = np.log(px_window / px_window.shift(1)).dropna()\n",
        "    log_ret = log_ret.dropna(axis=1, how=\"any\")\n",
        "\n",
        "    if log_ret.shape[0] < 10 or log_ret.shape[1] < 5:\n",
        "        return None, None, []\n",
        "\n",
        "    mu = log_ret.mean().values\n",
        "\n",
        "    lw = LedoitWolf().fit(log_ret.values)\n",
        "    cov = lw.covariance_\n",
        "    cov_psd = make_psd(cov)\n",
        "\n",
        "    tickers_final = log_ret.columns.tolist()\n",
        "    return mu, cov_psd, tickers_final\n",
        "\n",
        "def mean_variance_opt_full(mu, cov, max_weight=0.05, risk_aversion=1.0):\n",
        "    \"\"\"\n",
        "    Classic mean-variance problem:\n",
        "        max_w  μᵀw - λ wᵀΣw\n",
        "        s.t.   sum(w) = 1,  0 <= w <= max_weight\n",
        "    \"\"\"\n",
        "    mu = np.array(mu).ravel()\n",
        "    cov = np.array(cov)\n",
        "\n",
        "    n = len(mu)\n",
        "    w = cp.Variable(n)\n",
        "\n",
        "    cov_psd = make_psd(cov)\n",
        "    cov_psd = cp.psd_wrap(cov_psd)   # tells CVXPY the matrix is PSD\n",
        "\n",
        "    objective = cp.Maximize(mu @ w - risk_aversion * cp.quad_form(w, cov_psd))\n",
        "    constraints = [\n",
        "        cp.sum(w) == 1,\n",
        "        w >= 0,\n",
        "        w <= max_weight\n",
        "    ]\n",
        "\n",
        "    prob = cp.Problem(objective, constraints)\n",
        "    try:\n",
        "        prob.solve(solver=cp.SCS, verbose=False)\n",
        "    except Exception as e:\n",
        "        print(\"Optimizer error:\", e)\n",
        "        return np.ones(n) / n\n",
        "\n",
        "    if w.value is None:\n",
        "        return np.ones(n) / n\n",
        "\n",
        "    w_val = np.array(w.value).ravel()\n",
        "    w_val = np.clip(w_val, 0, None)\n",
        "    if w_val.sum() == 0:\n",
        "        w_val = np.ones(n) / n\n",
        "    else:\n",
        "        w_val /= w_val.sum()\n",
        "    return w_val\n",
        "\n",
        "def monte_carlo_sim(mu, cov, weights, horizon=21, n_sims=10000):\n",
        "    \"\"\"\n",
        "    Run a simple multivariate normal Monte Carlo on log returns.\n",
        "    Returns an array of final cumulative returns (one per simulation).\n",
        "    \"\"\"\n",
        "    mu = np.asarray(mu)\n",
        "    cov = np.asarray(cov)\n",
        "    weights = np.asarray(weights)\n",
        "\n",
        "    try:\n",
        "        sims = np.random.multivariate_normal(mu, cov, size=(n_sims, horizon))\n",
        "    except np.linalg.LinAlgError:\n",
        "        # If the covariance is still cranky, fall back to a diagonal version\n",
        "        diag_cov = np.diag(np.diag(cov))\n",
        "        sims = np.random.multivariate_normal(mu, diag_cov, size=(n_sims, horizon))\n",
        "\n",
        "    port_daily = sims @ weights\n",
        "    port_prices = np.exp(np.cumsum(port_daily, axis=1))\n",
        "    final_returns = port_prices[:, -1] - 1\n",
        "    return final_returns\n",
        "\n",
        "\n",
        "def plot_return_distribution(prices, out_dir):\n",
        "    \"\"\"\n",
        "    Quick sanity check: show the distribution of daily log returns across all assets.\n",
        "    \"\"\"\n",
        "    log_ret = np.log(prices / prices.shift(1)).stack().dropna()\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(log_ret, bins=100, density=True)\n",
        "    plt.title(\"Distribution of Daily Log Returns (All Assets)\")\n",
        "    plt.xlabel(\"Log Return\")\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_dir, \"dist_daily_log_returns.pdf\")\n",
        "    plt.savefig(fname)\n",
        "    plt.show()\n",
        "    print(\"Saved daily log-return distribution to\", fname)\n",
        "\n",
        "def plot_mc_sample(mc_returns, out_dir, tag):\n",
        "    \"\"\"\n",
        "    Plot a histogram of Monte Carlo final returns for one rebalance date.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(mc_returns, bins=100, density=True)\n",
        "    plt.title(f\"Sample Monte Carlo Final Return Distribution ({tag})\")\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_dir, f\"sample_mc_distribution_{tag}.pdf\")\n",
        "    plt.savefig(fname)\n",
        "    plt.show()\n",
        "    print(\"Saved sample MC distribution to\", fname)\n",
        "\n",
        "def plot_mc_paths(mu, cov, w, out_dir, tag, n_paths=25, horizon=63):\n",
        "    \"\"\"\n",
        "    Plot a small set of simulated paths to get a feel for path dispersion.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        sims = np.random.multivariate_normal(mu, cov, size=(n_paths, horizon))\n",
        "    except np.linalg.LinAlgError:\n",
        "        diag_cov = np.diag(np.diag(cov))\n",
        "        sims = np.random.multivariate_normal(mu, diag_cov, size=(n_paths, horizon))\n",
        "\n",
        "    port = np.exp(np.cumsum(sims @ w, axis=1))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(port.T, alpha=0.5)\n",
        "    plt.title(f\"Monte Carlo Price Paths (Sample, {tag})\")\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_dir, f\"mc_paths_{tag}.pdf\")\n",
        "    plt.savefig(fname)\n",
        "    plt.show()\n",
        "    print(\"Saved sample MC paths to\", fname)\n",
        "\n",
        "# One-time global return distribution plot\n",
        "plot_return_distribution(prices, OUT_RESULTS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INS664V2yNI5"
      },
      "source": [
        "**Weight snapshot utilities:**\n",
        "Helper functions to compress the per-day weights into rebalance snapshots and plot the top holdings over time and at the final rebalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2yRJ_feyM8I"
      },
      "outputs": [],
      "source": [
        "def compress_weights_to_rebalances(weights_hist, tol=1e-10):\n",
        "    \"\"\"\n",
        "    Take the full weight history (list of (date, weight_series)) and\n",
        "    keep only dates where the weight vector actually changes.\n",
        "    This makes it easier to plot rebalance-level holdings.\n",
        "    \"\"\"\n",
        "    if not weights_hist:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    all_tickers = sorted({t for _, w in weights_hist for t in w.index})\n",
        "    snaps = []\n",
        "    prev_vec = None\n",
        "\n",
        "    for d, w_ser in weights_hist:\n",
        "        w_full = w_ser.reindex(all_tickers).fillna(0)\n",
        "        vec = w_full.values\n",
        "\n",
        "        if prev_vec is None or not np.allclose(prev_vec, vec, atol=tol):\n",
        "            snaps.append((d, w_full))\n",
        "            prev_vec = vec.copy()\n",
        "\n",
        "    if not snaps:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    W = pd.DataFrame({d: w for d, w in snaps}).T\n",
        "    W.index.name = \"date\"\n",
        "    W = W.sort_index()\n",
        "    return W\n",
        "\n",
        "def plot_top_holdings(weights_hist, name, out_dir, top_n=15):\n",
        "    \"\"\"\n",
        "    Show the top-N tickers (by average weight) as a stacked area chart\n",
        "    over rebalance dates.\n",
        "    \"\"\"\n",
        "    W = compress_weights_to_rebalances(weights_hist)\n",
        "    if W.empty:\n",
        "        print(f\"[WARN] No weight snapshots for {name}\")\n",
        "        return\n",
        "\n",
        "    avg_w = W.mean().sort_values(ascending=False)\n",
        "    top_tickers = avg_w.head(top_n).index\n",
        "    W_top = W[top_tickers]\n",
        "\n",
        "    row_sums = W_top.sum(axis=1).replace(0, np.nan)\n",
        "    W_norm = W_top.div(row_sums, axis=0)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    W_norm.plot.area(ax=plt.gca(), linewidth=0)\n",
        "    plt.title(f\"{name}: Top {top_n} Holdings Over Rebalance Dates (weight share)\")\n",
        "    plt.ylabel(\"Weight Share\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.legend(loc=\"upper left\", fontsize=8, ncol=3)\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_dir, f\"{name}_top{top_n}_holdings_area.pdf\")\n",
        "    plt.savefig(fname)\n",
        "    plt.show()\n",
        "    print(f\"Saved holdings area chart to {fname}\")\n",
        "\n",
        "def plot_final_weights(weights_hist, name, out_dir, top_n=20):\n",
        "    \"\"\"\n",
        "    Bar chart of the final rebalance's top-N holdings.\n",
        "    \"\"\"\n",
        "    W = compress_weights_to_rebalances(weights_hist)\n",
        "    if W.empty:\n",
        "        print(f\"[WARN] No weight snapshots for {name}\")\n",
        "        return\n",
        "\n",
        "    w_last = W.iloc[-1].sort_values(ascending=False)\n",
        "    w_top = w_last.head(top_n)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    w_top.plot(kind=\"bar\")\n",
        "    plt.title(f\"{name}: Final Rebalance Weights (Top {top_n})\")\n",
        "    plt.ylabel(\"Weight\")\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_dir, f\"{name}_final_weights_top{top_n}.pdf\")\n",
        "    plt.savefig(fname)\n",
        "    plt.show()\n",
        "    print(f\"Saved final weights bar chart to {fname}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJmshST3yTEF"
      },
      "source": [
        "**Backtest engines:**\n",
        "Three backtest functions:\n",
        "Equal-weight portfolio,\n",
        "historical mean-variance,\n",
        "mean-variance blended with ML scores.\n",
        "All of them also run Monte Carlo at each rebalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP7573wmyVnR"
      },
      "outputs": [],
      "source": [
        "def backtest_equal_weight(prices,\n",
        "                          sp500_df,\n",
        "                          start_date, end_date,\n",
        "                          lookback_days=LOOKBACK_DAYS):\n",
        "\n",
        "    prices_bt = prices.loc[start_date:end_date]\n",
        "    dates = prices_bt.index\n",
        "\n",
        "    capital = 1.0\n",
        "    vals = []\n",
        "    weights_hist = []\n",
        "    mc_history = []\n",
        "\n",
        "    i = 0\n",
        "    first_mc_plotted = False\n",
        "    print(\"Running equal weight backtest\")\n",
        "    while i < len(dates):\n",
        "        reb_date = dates[i]\n",
        "\n",
        "        sp_members = get_sp500_members_on(reb_date, sp500_df)\n",
        "        universe = [\n",
        "            t for t in sp_members\n",
        "            if t in prices_bt.columns and prices_bt[t].notna().sum() > lookback_days\n",
        "        ]\n",
        "\n",
        "        mu_hist, cov, tickers_final = compute_mu_cov_for_universe(\n",
        "            prices_bt, universe, reb_date, lookback_days=lookback_days\n",
        "        )\n",
        "\n",
        "        if cov is None or len(tickers_final) < 5:\n",
        "            print(f\"[WARN] Skipping {reb_date.date()} — insufficient μ/Σ\")\n",
        "            i += REBALANCE_DAYS\n",
        "            continue\n",
        "\n",
        "        n = len(tickers_final)\n",
        "        w = np.ones(n) / n\n",
        "        w_ser = pd.Series(w, index=tickers_final)\n",
        "\n",
        "        mc_returns = monte_carlo_sim(\n",
        "            mu_hist, cov, w,\n",
        "            horizon=REBALANCE_DAYS,\n",
        "            n_sims=10000\n",
        "        )\n",
        "\n",
        "        mc_history.append({\n",
        "            \"rebalance_date\": reb_date,\n",
        "            \"mean\": mc_returns.mean(),\n",
        "            \"std\":  mc_returns.std(),\n",
        "            \"sharpe\": mc_returns.mean()/mc_returns.std() if mc_returns.std()>0 else np.nan,\n",
        "            \"VaR_5\":  np.percentile(mc_returns, 5),\n",
        "            \"CVaR_5\": mc_returns[mc_returns <= np.percentile(mc_returns, 5)].mean()\n",
        "        })\n",
        "\n",
        "        # Only draw MC charts on the first successful rebalance to avoid spamming\n",
        "        if not first_mc_plotted:\n",
        "            plot_mc_sample(mc_returns, OUT_RESULTS_DIR, \"eq\")\n",
        "            plot_mc_paths(mu_hist, cov, w, OUT_RESULTS_DIR, \"eq\",\n",
        "                          n_paths=25, horizon=REBALANCE_DAYS)\n",
        "            first_mc_plotted = True\n",
        "\n",
        "        hold_dates = dates[i : min(i + REBALANCE_DAYS, len(dates))]\n",
        "        px = prices_bt.loc[hold_dates, tickers_final]\n",
        "        lr = np.log(px / px.shift(1)).dropna()\n",
        "\n",
        "        port_lr = lr.values @ w_ser.values\n",
        "        for d, r in zip(lr.index, port_lr):\n",
        "            capital *= np.exp(r)\n",
        "            vals.append((d, capital))\n",
        "            weights_hist.append((d, w_ser))\n",
        "\n",
        "        i += REBALANCE_DAYS\n",
        "\n",
        "    port_val = pd.DataFrame(vals, columns=[\"date\", \"portfolio\"]).set_index(\"date\")\n",
        "    mc_history = pd.DataFrame(mc_history).set_index(\"rebalance_date\")\n",
        "    return port_val, weights_hist, mc_history\n",
        "\n",
        "def backtest_mv_hist(prices,\n",
        "                     sp500_df,\n",
        "                     start_date, end_date,\n",
        "                     lookback_days=LOOKBACK_DAYS,\n",
        "                     max_weight=MAX_WEIGHT,\n",
        "                     risk_aversion=1.0):\n",
        "\n",
        "    prices_bt = prices.loc[start_date:end_date]\n",
        "    dates = prices_bt.index\n",
        "\n",
        "    capital = 1.0\n",
        "    vals = []\n",
        "    weights_hist = []\n",
        "    mc_history = []\n",
        "\n",
        "    i = 0\n",
        "    first_mc_plotted = False\n",
        "    print(\"Running MV-Hist backtest\")\n",
        "    while i < len(dates):\n",
        "        reb_date = dates[i]\n",
        "\n",
        "        sp_members = get_sp500_members_on(reb_date, sp500_df)\n",
        "        universe = [\n",
        "            t for t in sp_members\n",
        "            if t in prices_bt.columns and prices_bt[t].notna().sum() > lookback_days\n",
        "        ]\n",
        "\n",
        "        mu_hist, cov, tickers_final = compute_mu_cov_for_universe(\n",
        "            prices_bt, universe, reb_date, lookback_days\n",
        "        )\n",
        "\n",
        "        if cov is None or len(tickers_final) < 5:\n",
        "            print(f\"[WARN] Skipping {reb_date.date()} — insufficient μ/Σ\")\n",
        "            i += REBALANCE_DAYS\n",
        "            continue\n",
        "\n",
        "        w = mean_variance_opt_full(mu_hist, cov,\n",
        "                                   max_weight=max_weight,\n",
        "                                   risk_aversion=risk_aversion)\n",
        "        w_ser = pd.Series(w, index=tickers_final)\n",
        "\n",
        "        mc_returns = monte_carlo_sim(mu_hist, cov, w,\n",
        "                                     horizon=REBALANCE_DAYS,\n",
        "                                     n_sims=10000)\n",
        "\n",
        "        mc_history.append({\n",
        "            \"rebalance_date\": reb_date,\n",
        "            \"mean\": mc_returns.mean(),\n",
        "            \"std\":  mc_returns.std(),\n",
        "            \"sharpe\": mc_returns.mean()/mc_returns.std() if mc_returns.std()>0 else np.nan,\n",
        "            \"VaR_5\":  np.percentile(mc_returns, 5),\n",
        "            \"CVaR_5\": mc_returns[mc_returns <= np.percentile(mc_returns, 5)].mean()\n",
        "        })\n",
        "\n",
        "        if not first_mc_plotted:\n",
        "            plot_mc_sample(mc_returns, OUT_RESULTS_DIR, \"mv_hist\")\n",
        "            plot_mc_paths(mu_hist, cov, w, OUT_RESULTS_DIR, \"mv_hist\",\n",
        "                          n_paths=25, horizon=REBALANCE_DAYS)\n",
        "            first_mc_plotted = True\n",
        "\n",
        "        hold_dates = dates[i : min(i + REBALANCE_DAYS, len(dates))]\n",
        "        px = prices_bt.loc[hold_dates, tickers_final]\n",
        "        lr = np.log(px / px.shift(1)).dropna()\n",
        "\n",
        "        port_lr = lr.values @ w_ser.values\n",
        "        for d, r in zip(lr.index, port_lr):\n",
        "            capital *= np.exp(r)\n",
        "            vals.append((d, capital))\n",
        "            weights_hist.append((d, w_ser))\n",
        "\n",
        "        i += REBALANCE_DAYS\n",
        "\n",
        "    port_val = pd.DataFrame(vals, columns=[\"date\", \"portfolio\"]).set_index(\"date\")\n",
        "    mc_history = pd.DataFrame(mc_history).set_index(\"rebalance_date\")\n",
        "    return port_val, weights_hist, mc_history\n",
        "\n",
        "def backtest_mv_ml(prices,\n",
        "                   sp500_df,\n",
        "                   features_all,\n",
        "                   ml_model,\n",
        "                   start_date, end_date,\n",
        "                   lookback_days=LOOKBACK_DAYS,\n",
        "                   max_weight=MAX_WEIGHT,\n",
        "                   alpha_ml=0.4,\n",
        "                   risk_aversion=1.0):\n",
        "\n",
        "    prices_bt = prices.loc[start_date:end_date]\n",
        "    dates = prices_bt.index\n",
        "\n",
        "    feats = features_all.copy()\n",
        "    feats.index = pd.to_datetime(feats.index)\n",
        "    feats = feats.sort_index()\n",
        "\n",
        "    capital = 1.0\n",
        "    vals = []\n",
        "    weights_hist = []\n",
        "    mc_summary = []\n",
        "\n",
        "    i = 0\n",
        "    first_mc_plotted = False\n",
        "    print(\"Running MV-ML backtest\")\n",
        "    while i < len(dates):\n",
        "        reb_date = dates[i]\n",
        "\n",
        "        sp_members = get_sp500_members_on(reb_date, sp500_df)\n",
        "        universe = sorted(set(sp_members) | set(ETF_TICKERS))\n",
        "        universe = [\n",
        "            t for t in universe\n",
        "            if t in prices_bt.columns and prices_bt[t].notna().sum() > lookback_days\n",
        "        ]\n",
        "\n",
        "        mu_hist, cov, tickers_final = compute_mu_cov_for_universe(\n",
        "            prices_bt, universe, reb_date, lookback_days=lookback_days\n",
        "        )\n",
        "        if cov is None or len(tickers_final) < 5:\n",
        "            print(f\"[WARN] Skipping {reb_date.date()} — insufficient μ/Σ\")\n",
        "            i += REBALANCE_DAYS\n",
        "            continue\n",
        "\n",
        "        # Get the latest feature row per ticker up to the rebalance date\n",
        "        rows = []\n",
        "        used_tickers = []\n",
        "        for t in tickers_final:\n",
        "            df_t = feats[(feats[\"ticker\"] == t) & (feats.index <= reb_date)]\n",
        "            if len(df_t) == 0:\n",
        "                continue\n",
        "            df_t = df_t.sort_index()\n",
        "            rows.append(df_t.iloc[-1])\n",
        "            used_tickers.append(t)\n",
        "\n",
        "        if len(used_tickers) < 5:\n",
        "            print(f\"[WARN] Skipping {reb_date.date()} — too few tickers with features\")\n",
        "            i += REBALANCE_DAYS\n",
        "            continue\n",
        "\n",
        "        X_reb = pd.DataFrame(rows)\n",
        "\n",
        "        # Align μ and Σ to the tickers we actually have features for\n",
        "        idx_map = {t: j for j, t in enumerate(tickers_final)}\n",
        "        idx = [idx_map[t] for t in used_tickers]\n",
        "\n",
        "        mu_hist_used = mu_hist[idx]\n",
        "        cov_used     = cov[np.ix_(idx, idx)]\n",
        "\n",
        "        if np.isnan(mu_hist_used).any() or np.isnan(cov_used).any():\n",
        "            print(f\"[WARN] Skipping {reb_date.date()} — NaNs in μ or Σ\")\n",
        "            i += REBALANCE_DAYS\n",
        "            continue\n",
        "\n",
        "        # ML alpha predictions\n",
        "        mu_ml_raw = ml_model.predict(X_reb)\n",
        "        mu_ml_raw = np.array(mu_ml_raw)\n",
        "\n",
        "        # Turn ML scores into ranks (0–1)\n",
        "        ml_mean = mu_ml_raw.mean()\n",
        "        ml_std  = mu_ml_raw.std() if mu_ml_raw.std() > 0 else 1.0\n",
        "        z_ml = (mu_ml_raw - ml_mean) / ml_std\n",
        "        ranks = pd.Series(z_ml).rank(method=\"average\") / len(z_ml)\n",
        "        ml_score = ranks.values  # higher = better\n",
        "\n",
        "        # Turn historical μ into ranks too\n",
        "        mu_hist_used = np.array(mu_hist_used)\n",
        "        h_mean = mu_hist_used.mean()\n",
        "        h_std  = mu_hist_used.std() if mu_hist_used.std() > 0 else 1.0\n",
        "        z_hist = (mu_hist_used - h_mean) / h_std\n",
        "        hist_score = (pd.Series(z_hist).rank(method=\"average\") / len(z_hist)).values\n",
        "\n",
        "        # Blend ML vs historical info\n",
        "        score_blend = alpha_ml * ml_score + (1 - alpha_ml) * hist_score\n",
        "\n",
        "        # Optionally keep only top-K names by blended score\n",
        "        if TOP_K is not None and len(used_tickers) > TOP_K:\n",
        "            order = np.argsort(-score_blend)  # sort descending\n",
        "            top_idx = order[:TOP_K]\n",
        "            used_tickers = [used_tickers[j] for j in top_idx]\n",
        "            mu_hist_used = mu_hist_used[top_idx]\n",
        "            cov_used     = cov_used[np.ix_(top_idx, top_idx)]\n",
        "            score_blend  = score_blend[top_idx]\n",
        "\n",
        "        # Convert scores into pseudo-returns on roughly the same scale as μ_hist_used\n",
        "        pseudo_mu = score_blend.copy()\n",
        "        pseudo_mu = pseudo_mu - pseudo_mu.mean()\n",
        "        pseudo_mu = pseudo_mu / (pseudo_mu.std() + 1e-8)\n",
        "        pseudo_mu = pseudo_mu * (mu_hist_used.std() + 1e-8) + mu_hist_used.mean()\n",
        "\n",
        "        # Solve mean-variance on the blended signal\n",
        "        w = mean_variance_opt_full(pseudo_mu, cov_used,\n",
        "                                   max_weight=max_weight,\n",
        "                                   risk_aversion=risk_aversion)\n",
        "        w_ser = pd.Series(w, index=used_tickers)\n",
        "\n",
        "        # Monte Carlo summary for this rebalance\n",
        "        mc_returns = monte_carlo_sim(\n",
        "            pseudo_mu,\n",
        "            cov_used,\n",
        "            w,\n",
        "            horizon=REBALANCE_DAYS,\n",
        "            n_sims=10000\n",
        "        )\n",
        "        mc_summary.append({\n",
        "            \"rebalance_date\": reb_date,\n",
        "            \"mean\": mc_returns.mean(),\n",
        "            \"std\": mc_returns.std(),\n",
        "            \"sharpe\": mc_returns.mean() / mc_returns.std() if mc_returns.std() > 0 else np.nan,\n",
        "            \"VaR_5\": np.percentile(mc_returns, 5),\n",
        "            \"CVaR_5\": mc_returns[mc_returns <= np.percentile(mc_returns, 5)].mean()\n",
        "        })\n",
        "\n",
        "        if not first_mc_plotted:\n",
        "            plot_mc_sample(mc_returns, OUT_RESULTS_DIR, \"mv_ml\")\n",
        "            plot_mc_paths(pseudo_mu, cov_used, w, OUT_RESULTS_DIR, \"mv_ml\",\n",
        "                          n_paths=25, horizon=REBALANCE_DAYS)\n",
        "            first_mc_plotted = True\n",
        "\n",
        "        hold_dates = dates[i : min(i + REBALANCE_DAYS, len(dates))]\n",
        "        px = prices_bt.loc[hold_dates, used_tickers]\n",
        "        lr = np.log(px / px.shift(1)).dropna()\n",
        "\n",
        "        port_lr = lr.values @ w_ser.values\n",
        "        for d, r in zip(lr.index, port_lr):\n",
        "            capital *= np.exp(r)\n",
        "            vals.append((d, capital))\n",
        "            weights_hist.append((d, w_ser))\n",
        "\n",
        "        i += REBALANCE_DAYS\n",
        "\n",
        "    port_val = pd.DataFrame(vals, columns=[\"date\", \"portfolio\"]).set_index(\"date\")\n",
        "    mc_summary = pd.DataFrame(mc_summary).set_index(\"rebalance_date\")\n",
        "\n",
        "    return port_val, weights_hist, mc_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKscplDQyc8i"
      },
      "source": [
        "**Run all three strategies:**\n",
        "Execute the equal-weight, historical mean-variance, and ML-enhanced mean-variance backtests over the chosen period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYGvDQoUyfsd"
      },
      "outputs": [],
      "source": [
        "port_eq,   w_eq,   mc_eq   = backtest_equal_weight(\n",
        "    prices, sp500_df,\n",
        "    start_date=BACKTEST_START,\n",
        "    end_date=BACKTEST_END\n",
        ")\n",
        "\n",
        "port_hist, w_hist, mc_hist = backtest_mv_hist(\n",
        "    prices, sp500_df,\n",
        "    start_date=BACKTEST_START,\n",
        "    end_date=BACKTEST_END\n",
        ")\n",
        "\n",
        "port_ml,   w_ml,   mc_ml   = backtest_mv_ml(\n",
        "    prices, sp500_df,\n",
        "    features_all,\n",
        "    best_ml_model,\n",
        "    start_date=BACKTEST_START,\n",
        "    end_date=BACKTEST_END\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRNCJ4p_yims"
      },
      "source": [
        "**Save results and basic performance stats:**\n",
        "Write portfolio paths, Monte Carlo summaries, and weights to disk. Then compute returns/volatility/Sharpe/drawdown and plot the combined equity curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_Izu59Wykx1"
      },
      "outputs": [],
      "source": [
        "# Save portfolio time series\n",
        "port_eq.to_csv(os.path.join(OUT_RESULTS_DIR, \"port_eq.csv\"))\n",
        "port_hist.to_csv(os.path.join(OUT_RESULTS_DIR, \"port_mv_hist.csv\"))\n",
        "port_ml.to_csv(os.path.join(OUT_RESULTS_DIR, \"port_mv_ml.csv\"))\n",
        "\n",
        "# Save Monte Carlo summaries\n",
        "mc_eq.to_csv(os.path.join(OUT_RESULTS_DIR, \"mc_eq_summary.csv\"))\n",
        "mc_hist.to_csv(os.path.join(OUT_RESULTS_DIR, \"mc_mv_hist_summary.csv\"))\n",
        "mc_ml.to_csv(os.path.join(OUT_RESULTS_DIR, \"mc_mv_ml_summary.csv\"))\n",
        "\n",
        "# Save weight histories (full objects) as pickles\n",
        "with open(os.path.join(OUT_RESULTS_DIR, \"weights_eq.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(w_eq, f)\n",
        "with open(os.path.join(OUT_RESULTS_DIR, \"weights_mv_hist.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(w_hist, f)\n",
        "with open(os.path.join(OUT_RESULTS_DIR, \"weights_mv_ml.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(w_ml, f)\n",
        "\n",
        "def compute_stats(port_val, freq=252):\n",
        "    \"\"\"\n",
        "    Compute some basic performance stats from a portfolio value series.\n",
        "    \"\"\"\n",
        "    v = port_val[\"portfolio\"]\n",
        "    rets = np.log(v / v.shift(1)).dropna()\n",
        "    total_ret = v.iloc[-1] - 1\n",
        "    ann_ret = rets.mean() * freq\n",
        "    ann_vol = rets.std() * np.sqrt(freq)\n",
        "    sharpe = ann_ret / ann_vol if ann_vol > 0 else np.nan\n",
        "    cummax = v.cummax()\n",
        "    dd = (v - cummax) / cummax\n",
        "    mdd = dd.min()\n",
        "    return {\n",
        "        \"Total Return\": float(total_ret),\n",
        "        \"Ann. Return\": float(ann_ret),\n",
        "        \"Ann. Vol\": float(ann_vol),\n",
        "        \"Sharpe\": float(sharpe),\n",
        "        \"Max Drawdown\": float(mdd),\n",
        "    }\n",
        "\n",
        "stats_eq   = compute_stats(port_eq)\n",
        "stats_hist = compute_stats(port_hist)\n",
        "stats_ml   = compute_stats(port_ml)\n",
        "\n",
        "print(\"Equal Weight:\", stats_eq)\n",
        "print(\"MV-Hist    :\", stats_hist)\n",
        "print(\"MV-ML      :\", stats_ml)\n",
        "\n",
        "# Plot the three equity curves together\n",
        "df_plot = pd.concat([\n",
        "    port_eq[\"portfolio\"].rename(\"Equal Weight\"),\n",
        "    port_hist[\"portfolio\"].rename(\"MV-Hist\"),\n",
        "    port_ml[\"portfolio\"].rename(\"MV-ML\"),\n",
        "], axis=1)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "df_plot.plot(ax=plt.gca(), title=\"Portfolio Value Over Time\")\n",
        "plt.ylabel(\"Portfolio Value\")\n",
        "plt.tight_layout()\n",
        "eq_curve_path = os.path.join(OUT_RESULTS_DIR, \"portfolio_equity_curves.pdf\")\n",
        "plt.savefig(eq_curve_path)\n",
        "plt.show()\n",
        "print(\"Saved equity curves to\", eq_curve_path)\n",
        "\n",
        "# Save stats as a small table\n",
        "stats_df = pd.DataFrame({\n",
        "    \"Equal_Weight\": stats_eq,\n",
        "    \"MV_Hist\": stats_hist,\n",
        "    \"MV_ML\": stats_ml\n",
        "})\n",
        "stats_df.to_csv(os.path.join(OUT_RESULTS_DIR, \"strategy_stats.csv\"))\n",
        "print(\"\\nSaved stats to:\", os.path.join(OUT_RESULTS_DIR, \"strategy_stats.csv\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfiaiecFyoqB"
      },
      "source": [
        "**Visualizations:**\n",
        "Plot how the top holdings evolve over time and what the final rebalance portfolio looks like for each strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5u5Akh5ysaw"
      },
      "outputs": [],
      "source": [
        "plot_top_holdings(w_eq,   \"Equal_Weight\", OUT_RESULTS_DIR, top_n=15)\n",
        "plot_final_weights(w_eq,  \"Equal_Weight\", OUT_RESULTS_DIR, top_n=20)\n",
        "\n",
        "plot_top_holdings(w_hist, \"MV_Hist\", OUT_RESULTS_DIR, top_n=15)\n",
        "plot_final_weights(w_hist,\"MV_Hist\", OUT_RESULTS_DIR, top_n=20)\n",
        "\n",
        "plot_top_holdings(w_ml,   \"MV_ML\", OUT_RESULTS_DIR, top_n=15)\n",
        "plot_final_weights(w_ml,  \"MV_ML\", OUT_RESULTS_DIR, top_n=20)\n",
        "\n",
        "print(\"\\nAll outputs saved under:\", OUT_RESULTS_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
